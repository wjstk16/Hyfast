{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  4\n",
      "Current cuda device  3\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 현재 Setup 되어있는 device 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  3\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.9 GB\n",
      "Cached:    6.0 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 3 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "# Additional Infos\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(GPU_NUM))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(GPU_NUM)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(GPU_NUM)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch, loss: 1.0275311470031738\n",
      "100th epoch, loss: 0.35861801330815724\n",
      "200th epoch, loss: 0.20775485351879144\n",
      "300th epoch, loss: 0.1784325284490592\n",
      "400th epoch, loss: 0.1674838155822686\n",
      "500th epoch, loss: 0.161346000875971\n",
      "600th epoch, loss: 0.15752158809268735\n",
      "700th epoch, loss: 0.15284418833847724\n",
      "800th epoch, loss: 0.14897684097834124\n",
      "900th epoch, loss: 0.14419028775077644\n",
      "1000th epoch, loss: 0.1450391133236209\n",
      "1100th epoch, loss: 0.1369074628899946\n",
      "1200th epoch, loss: 0.13241042650442617\n",
      "1300th epoch, loss: 0.1282188738346049\n",
      "1400th epoch, loss: 0.12458417310512405\n",
      "1500th epoch, loss: 0.12128327690232871\n",
      "1600th epoch, loss: 0.11764910768377722\n",
      "1700th epoch, loss: 0.11453489023227147\n",
      "1800th epoch, loss: 0.11162115094949106\n",
      "1900th epoch, loss: 0.1090875040084005\n",
      "2000th epoch, loss: 0.10705349386951414\n",
      "2100th epoch, loss: 0.10459560420708415\n",
      "2200th epoch, loss: 0.10285275843784884\n",
      "2300th epoch, loss: 0.10105758835332557\n",
      "2400th epoch, loss: 0.09940188696515094\n",
      "2500th epoch, loss: 0.09782341078881193\n",
      "2600th epoch, loss: 0.0963399903926513\n",
      "2700th epoch, loss: 0.09520774288799874\n",
      "2800th epoch, loss: 0.09359004298037082\n",
      "2900th epoch, loss: 0.09201036329257342\n",
      "3000th epoch, loss: 0.09063000716873987\n",
      "3100th epoch, loss: 0.08926662462779669\n",
      "3200th epoch, loss: 0.08786224514181823\n",
      "3300th epoch, loss: 0.08642475753739164\n",
      "3400th epoch, loss: 0.08494511529966274\n",
      "3500th epoch, loss: 0.0833625935402713\n",
      "3600th epoch, loss: 0.08175095926791075\n",
      "3700th epoch, loss: 0.08021178250010162\n",
      "3800th epoch, loss: 0.07850662211786665\n",
      "3900th epoch, loss: 0.07698266064045654\n",
      "4000th epoch, loss: 0.07539691807863683\n",
      "4100th epoch, loss: 0.07382466951512656\n",
      "4200th epoch, loss: 0.0724320005783071\n",
      "4300th epoch, loss: 0.07101133721491042\n",
      "4400th epoch, loss: 0.06974960588032617\n",
      "4500th epoch, loss: 0.06861682291345056\n",
      "4600th epoch, loss: 0.06739167310574191\n",
      "4700th epoch, loss: 0.06618217036626542\n",
      "4800th epoch, loss: 0.06476399641866822\n",
      "4900th epoch, loss: 0.06395927015500125\n",
      "5000th epoch, loss: 0.06246634151490225\n",
      "5100th epoch, loss: 0.061377988981171645\n",
      "5200th epoch, loss: 0.06017731908097394\n",
      "5300th epoch, loss: 0.05889319362394682\n",
      "5400th epoch, loss: 0.05806769283052726\n",
      "5500th epoch, loss: 0.05708342379642292\n",
      "5600th epoch, loss: 0.05582775278029423\n",
      "5700th epoch, loss: 0.05491270479999019\n",
      "5800th epoch, loss: 0.0543343538121334\n",
      "5900th epoch, loss: 0.05309109340791342\n",
      "6000th epoch, loss: 0.05200447939359818\n",
      "6100th epoch, loss: 0.05140612284588372\n",
      "6200th epoch, loss: 0.05028995119442214\n",
      "6300th epoch, loss: 0.04950442566854826\n",
      "6400th epoch, loss: 0.04876919570836931\n",
      "6500th epoch, loss: 0.04801271735153354\n",
      "6600th epoch, loss: 0.04706960925559411\n",
      "6700th epoch, loss: 0.04631221200038816\n",
      "6800th epoch, loss: 0.04564299820669606\n",
      "6900th epoch, loss: 0.04518199531353696\n",
      "7000th epoch, loss: 0.04432728379498003\n",
      "7100th epoch, loss: 0.043763717328072764\n",
      "7200th epoch, loss: 0.04303575638249427\n",
      "7300th epoch, loss: 0.04260434842064498\n",
      "7400th epoch, loss: 0.04198428290510794\n",
      "7500th epoch, loss: 0.041287132659252816\n",
      "7600th epoch, loss: 0.040957935473432606\n",
      "7700th epoch, loss: 0.040227771041212806\n",
      "7800th epoch, loss: 0.03973083274025539\n",
      "7900th epoch, loss: 0.03961405078895524\n",
      "8000th epoch, loss: 0.038770869915049404\n",
      "8100th epoch, loss: 0.0385285165547495\n",
      "8200th epoch, loss: 0.03790143490463228\n",
      "8300th epoch, loss: 0.03746762638051002\n",
      "8400th epoch, loss: 0.0372119335522271\n",
      "8500th epoch, loss: 0.036667369462631876\n",
      "8600th epoch, loss: 0.036397741250969416\n",
      "8700th epoch, loss: 0.03593764073010769\n",
      "8800th epoch, loss: 0.03560581846026456\n",
      "8900th epoch, loss: 0.035346031256100086\n",
      "9000th epoch, loss: 0.034941675746845945\n",
      "9100th epoch, loss: 0.034662826355642974\n",
      "9200th epoch, loss: 0.03418715247807061\n",
      "9300th epoch, loss: 0.033877908961731984\n",
      "9400th epoch, loss: 0.03359746955428084\n",
      "9500th epoch, loss: 0.033356606636135884\n",
      "9600th epoch, loss: 0.03301703921514408\n",
      "9700th epoch, loss: 0.032667919286072065\n",
      "9800th epoch, loss: 0.03247515079831922\n",
      "9900th epoch, loss: 0.03206512778180562\n",
      "10000th epoch, loss: 0.031843672851422256\n",
      "10100th epoch, loss: 0.03156042240522799\n",
      "10200th epoch, loss: 0.031229002407633263\n",
      "10300th epoch, loss: 0.03105232686369509\n",
      "10400th epoch, loss: 0.030797614975760224\n",
      "10500th epoch, loss: 0.03050879833256272\n",
      "10600th epoch, loss: 0.030344516140042285\n",
      "10700th epoch, loss: 0.029987787821449365\n",
      "10800th epoch, loss: 0.029773211331542475\n",
      "10900th epoch, loss: 0.029666956705748343\n",
      "11000th epoch, loss: 0.0295645480943066\n",
      "11100th epoch, loss: 0.029290495107625658\n",
      "11200th epoch, loss: 0.028742922911542068\n",
      "11300th epoch, loss: 0.028521386409122863\n",
      "11400th epoch, loss: 0.028342204498838012\n",
      "11500th epoch, loss: 0.02814047562976805\n",
      "11600th epoch, loss: 0.027899341672087935\n",
      "11700th epoch, loss: 0.027675006570077904\n",
      "11800th epoch, loss: 0.027393332232847373\n",
      "11900th epoch, loss: 0.027285468924537294\n",
      "12000th epoch, loss: 0.027163310988743222\n",
      "12100th epoch, loss: 0.026927489511565227\n",
      "12200th epoch, loss: 0.026672491998731215\n",
      "12300th epoch, loss: 0.026423439977043675\n",
      "12400th epoch, loss: 0.026276383934136775\n",
      "12500th epoch, loss: 0.02624307163556123\n",
      "12600th epoch, loss: 0.026082133978304473\n",
      "12700th epoch, loss: 0.025973961398498984\n",
      "12800th epoch, loss: 0.02560952648012566\n",
      "12900th epoch, loss: 0.025415982794347747\n",
      "13000th epoch, loss: 0.025332322823491245\n",
      "13100th epoch, loss: 0.02532549680979528\n",
      "13200th epoch, loss: 0.0250658357611731\n",
      "13300th epoch, loss: 0.024792784205026154\n",
      "13400th epoch, loss: 0.024643812357272848\n",
      "13500th epoch, loss: 0.024550711378369797\n",
      "13600th epoch, loss: 0.02435821967561174\n",
      "13700th epoch, loss: 0.024204883166001955\n",
      "13800th epoch, loss: 0.02412080267781288\n",
      "13900th epoch, loss: 0.02393015264921983\n",
      "14000th epoch, loss: 0.02385522611982665\n",
      "14100th epoch, loss: 0.023608224105704064\n",
      "14200th epoch, loss: 0.023547867118765296\n",
      "14300th epoch, loss: 0.023426034761657354\n",
      "14400th epoch, loss: 0.02324715499829166\n",
      "14500th epoch, loss: 0.023151411182639994\n",
      "14600th epoch, loss: 0.023312169839274567\n",
      "14700th epoch, loss: 0.022920884960330283\n",
      "14800th epoch, loss: 0.022814700891036238\n",
      "14900th epoch, loss: 0.022767749889287023\n",
      "15000th epoch, loss: 0.022451855755492946\n",
      "15100th epoch, loss: 0.022603009226862387\n",
      "15200th epoch, loss: 0.022361400108200848\n",
      "15300th epoch, loss: 0.022247966009601397\n",
      "15400th epoch, loss: 0.022204494720078884\n",
      "15500th epoch, loss: 0.022003257623041596\n",
      "15600th epoch, loss: 0.021841008744036973\n",
      "15700th epoch, loss: 0.02179297419321906\n",
      "15800th epoch, loss: 0.021756390580844942\n",
      "15900th epoch, loss: 0.021781544301942353\n",
      "16000th epoch, loss: 0.02139968791207703\n",
      "16100th epoch, loss: 0.02134585520165214\n",
      "16200th epoch, loss: 0.021416774108825178\n",
      "16300th epoch, loss: 0.021092971053895185\n",
      "16400th epoch, loss: 0.021161049253636443\n",
      "16500th epoch, loss: 0.021183360478988292\n",
      "16600th epoch, loss: 0.021019190447183943\n",
      "16700th epoch, loss: 0.02093996968081512\n",
      "16800th epoch, loss: 0.020759998081695583\n",
      "16900th epoch, loss: 0.020959937885006327\n",
      "17000th epoch, loss: 0.02062766043708519\n",
      "17100th epoch, loss: 0.020478189062416698\n",
      "17200th epoch, loss: 0.020484316470631307\n",
      "17300th epoch, loss: 0.02020752657199065\n",
      "17400th epoch, loss: 0.02030704437597871\n",
      "17500th epoch, loss: 0.020291644494544056\n",
      "17600th epoch, loss: 0.0200160697853767\n",
      "17700th epoch, loss: 0.020010206160709698\n",
      "17800th epoch, loss: 0.019799386962300557\n",
      "17900th epoch, loss: 0.0197855206069663\n",
      "18000th epoch, loss: 0.01964937250033109\n",
      "18100th epoch, loss: 0.01977802743361428\n",
      "18200th epoch, loss: 0.01960203218023373\n",
      "18300th epoch, loss: 0.019495661050251648\n",
      "18400th epoch, loss: 0.019529611210010192\n",
      "18500th epoch, loss: 0.01934086297211958\n",
      "18600th epoch, loss: 0.019207006155217605\n",
      "18700th epoch, loss: 0.019223633334129013\n",
      "18800th epoch, loss: 0.019099100059898604\n",
      "18900th epoch, loss: 0.01903583735964912\n",
      "19000th epoch, loss: 0.018918080363482387\n",
      "19100th epoch, loss: 0.018889323204423537\n",
      "19200th epoch, loss: 0.018840011497159147\n",
      "19300th epoch, loss: 0.018654459709373034\n",
      "19400th epoch, loss: 0.018675701542275408\n",
      "19500th epoch, loss: 0.01880388588813414\n",
      "19600th epoch, loss: 0.018602154112318803\n",
      "19700th epoch, loss: 0.018532787320682288\n",
      "19800th epoch, loss: 0.018513559668844975\n",
      "19900th epoch, loss: 0.018291262014153396\n",
      "20000th epoch, loss: 0.018305416461858688\n",
      "20100th epoch, loss: 0.018192836126905428\n",
      "20200th epoch, loss: 0.018154612193951165\n",
      "20300th epoch, loss: 0.01817085623373691\n",
      "20400th epoch, loss: 0.018210336486777008\n",
      "20500th epoch, loss: 0.017957749505762614\n",
      "20600th epoch, loss: 0.017925917263152128\n",
      "20700th epoch, loss: 0.017811377031837782\n",
      "20800th epoch, loss: 0.017868024373534363\n",
      "20900th epoch, loss: 0.01788664870072525\n",
      "21000th epoch, loss: 0.017685475637234372\n",
      "21100th epoch, loss: 0.017596716667686706\n",
      "21200th epoch, loss: 0.017527828489807426\n",
      "21300th epoch, loss: 0.017854687810650666\n",
      "21400th epoch, loss: 0.017417610492685955\n",
      "21500th epoch, loss: 0.017581236044484985\n",
      "21600th epoch, loss: 0.0173351561612354\n",
      "21700th epoch, loss: 0.017357948993358584\n",
      "21800th epoch, loss: 0.017477934432308443\n",
      "21900th epoch, loss: 0.017162194601379327\n",
      "22000th epoch, loss: 0.017157433979630987\n",
      "22100th epoch, loss: 0.017110332242572614\n",
      "22200th epoch, loss: 0.017013405001394175\n",
      "22300th epoch, loss: 0.016915614914151333\n",
      "22400th epoch, loss: 0.01698156134225101\n",
      "22500th epoch, loss: 0.017138870410506872\n",
      "22600th epoch, loss: 0.016799789150979277\n",
      "22700th epoch, loss: 0.016795017765004493\n",
      "22800th epoch, loss: 0.016755006935863244\n",
      "22900th epoch, loss: 0.016611461372419287\n",
      "23000th epoch, loss: 0.016570254489013947\n",
      "23100th epoch, loss: 0.01664503246088454\n",
      "23200th epoch, loss: 0.016545072383563654\n",
      "23300th epoch, loss: 0.016478148274551376\n",
      "23400th epoch, loss: 0.01646301455635609\n",
      "23500th epoch, loss: 0.016397479989675046\n",
      "23600th epoch, loss: 0.016352377993354945\n",
      "23700th epoch, loss: 0.01618160192806288\n",
      "23800th epoch, loss: 0.01632315300978355\n",
      "23900th epoch, loss: 0.01609240054262593\n",
      "24000th epoch, loss: 0.01633748534757075\n",
      "24100th epoch, loss: 0.016026561789887095\n",
      "24200th epoch, loss: 0.016103723171132667\n",
      "24300th epoch, loss: 0.01609289658272599\n",
      "24400th epoch, loss: 0.015892421821151116\n",
      "24500th epoch, loss: 0.01587358827109869\n",
      "24600th epoch, loss: 0.015846431463781757\n",
      "24700th epoch, loss: 0.01589578564755583\n",
      "24800th epoch, loss: 0.015747023736059282\n",
      "24900th epoch, loss: 0.015651990537516362\n",
      "25000th epoch, loss: 0.01599258511829695\n",
      "25100th epoch, loss: 0.01566045381744856\n",
      "25200th epoch, loss: 0.015513169120310297\n",
      "25300th epoch, loss: 0.015861880598581193\n",
      "25400th epoch, loss: 0.015562699531844163\n",
      "25500th epoch, loss: 0.015524056063206192\n",
      "25600th epoch, loss: 0.015594458054449855\n",
      "25700th epoch, loss: 0.015315822608281123\n",
      "25800th epoch, loss: 0.015546697924543995\n",
      "25900th epoch, loss: 0.015319988524951192\n",
      "26000th epoch, loss: 0.015192949357047665\n",
      "26100th epoch, loss: 0.015122601282273836\n",
      "26200th epoch, loss: 0.015201222960236349\n",
      "26300th epoch, loss: 0.015177556342407757\n",
      "26400th epoch, loss: 0.015145536552735771\n",
      "26500th epoch, loss: 0.015050161112961303\n",
      "26600th epoch, loss: 0.015042120321773045\n",
      "26700th epoch, loss: 0.015232350778762789\n",
      "26800th epoch, loss: 0.014893398972709929\n",
      "26900th epoch, loss: 0.014991704991575778\n",
      "27000th epoch, loss: 0.015053387444994452\n",
      "27100th epoch, loss: 0.014896593328825574\n",
      "27200th epoch, loss: 0.014896920751333018\n",
      "27300th epoch, loss: 0.014881311509261091\n",
      "27400th epoch, loss: 0.014733951979719776\n",
      "27500th epoch, loss: 0.014798276026542838\n",
      "27600th epoch, loss: 0.014703649053803565\n",
      "27700th epoch, loss: 0.014680946877787468\n",
      "27800th epoch, loss: 0.014572034226847122\n",
      "27900th epoch, loss: 0.014513864266372339\n",
      "28000th epoch, loss: 0.014655901661777129\n",
      "28100th epoch, loss: 0.014464910322360587\n",
      "28200th epoch, loss: 0.0145379561550213\n",
      "28300th epoch, loss: 0.014386536336762668\n",
      "28400th epoch, loss: 0.01443138268664329\n",
      "28500th epoch, loss: 0.014472697519113082\n",
      "28600th epoch, loss: 0.014314384650684436\n",
      "28700th epoch, loss: 0.014392211151591046\n",
      "28800th epoch, loss: 0.014305826638278844\n",
      "28900th epoch, loss: 0.014295410711393033\n",
      "29000th epoch, loss: 0.014472647729358492\n",
      "29100th epoch, loss: 0.014206349106362783\n",
      "29200th epoch, loss: 0.014105204509680731\n",
      "29300th epoch, loss: 0.0141718729244605\n",
      "29400th epoch, loss: 0.014121882736255205\n",
      "29500th epoch, loss: 0.014197275659915017\n",
      "29600th epoch, loss: 0.01409448347946564\n",
      "29700th epoch, loss: 0.014030249379836247\n",
      "29800th epoch, loss: 0.01402836055409033\n",
      "29900th epoch, loss: 0.013878575863644282\n",
      "30000th epoch, loss: 0.013906426340410748\n",
      "30100th epoch, loss: 0.013955877346249541\n",
      "30200th epoch, loss: 0.014119601526205904\n",
      "30300th epoch, loss: 0.01390220143699982\n",
      "30400th epoch, loss: 0.013774720015287395\n",
      "30500th epoch, loss: 0.013852696354590353\n",
      "30600th epoch, loss: 0.013680677748338404\n",
      "30700th epoch, loss: 0.013665916614152356\n",
      "30800th epoch, loss: 0.013824222115444831\n",
      "30900th epoch, loss: 0.013730801481974381\n",
      "31000th epoch, loss: 0.013718780217224471\n",
      "31100th epoch, loss: 0.013543911072343144\n",
      "31200th epoch, loss: 0.01364640316427683\n",
      "31300th epoch, loss: 0.013825797892512474\n",
      "31400th epoch, loss: 0.013578182599889176\n",
      "31500th epoch, loss: 0.013537949953402751\n",
      "31600th epoch, loss: 0.013470518988679925\n",
      "31700th epoch, loss: 0.013453214835488104\n",
      "31800th epoch, loss: 0.013383333368534775\n",
      "31900th epoch, loss: 0.013344725640149603\n",
      "32000th epoch, loss: 0.013386422494063237\n",
      "32100th epoch, loss: 0.013640348982836824\n",
      "32200th epoch, loss: 0.013304144908220534\n",
      "32300th epoch, loss: 0.01326774122829447\n",
      "32400th epoch, loss: 0.01344466815993292\n",
      "32500th epoch, loss: 0.01328030850926801\n",
      "32600th epoch, loss: 0.013262832138692572\n",
      "32700th epoch, loss: 0.013398787220128059\n",
      "32800th epoch, loss: 0.01313603155378122\n",
      "32900th epoch, loss: 0.013150013510652193\n",
      "33000th epoch, loss: 0.013177809165643706\n",
      "33100th epoch, loss: 0.01317473336676811\n",
      "33200th epoch, loss: 0.013106883453616214\n",
      "33300th epoch, loss: 0.01301878657609927\n",
      "33400th epoch, loss: 0.013015136593976\n",
      "33500th epoch, loss: 0.013078157492176794\n",
      "33600th epoch, loss: 0.012972333305099287\n",
      "33700th epoch, loss: 0.012936094240193792\n",
      "33800th epoch, loss: 0.012907751312417297\n",
      "33900th epoch, loss: 0.012922075394360933\n",
      "34000th epoch, loss: 0.013171859445944895\n",
      "34100th epoch, loss: 0.013002510326777409\n",
      "34200th epoch, loss: 0.012793767409886985\n",
      "34300th epoch, loss: 0.012967097590815532\n",
      "34400th epoch, loss: 0.012768538748126456\n",
      "34500th epoch, loss: 0.01279722022766364\n",
      "34600th epoch, loss: 0.01278856616426695\n",
      "34700th epoch, loss: 0.01269608119410553\n",
      "34800th epoch, loss: 0.012883454449066222\n",
      "34900th epoch, loss: 0.012658926742713229\n",
      "35000th epoch, loss: 0.012649717011882482\n",
      "35100th epoch, loss: 0.012705595726270905\n",
      "35200th epoch, loss: 0.01273621529819485\n",
      "35300th epoch, loss: 0.0127725801486059\n",
      "35400th epoch, loss: 0.012559539579298742\n",
      "35500th epoch, loss: 0.01260291260914348\n",
      "35600th epoch, loss: 0.012587926717591307\n",
      "35700th epoch, loss: 0.012622068232339748\n",
      "35800th epoch, loss: 0.012541658496558646\n",
      "35900th epoch, loss: 0.012500348496255129\n",
      "36000th epoch, loss: 0.012505592559220272\n",
      "36100th epoch, loss: 0.012474024086350552\n",
      "36200th epoch, loss: 0.012468924013296273\n",
      "36300th epoch, loss: 0.012381624273819508\n",
      "36400th epoch, loss: 0.01249014641286776\n",
      "36500th epoch, loss: 0.01236011143454119\n",
      "36600th epoch, loss: 0.01231957177359769\n",
      "36700th epoch, loss: 0.012280865740260175\n",
      "36800th epoch, loss: 0.012277481827778094\n",
      "36900th epoch, loss: 0.012399329260799336\n",
      "37000th epoch, loss: 0.012237078243161349\n",
      "37100th epoch, loss: 0.012194088490290089\n",
      "37200th epoch, loss: 0.012362381861499159\n",
      "37300th epoch, loss: 0.012217655318181173\n",
      "37400th epoch, loss: 0.012153819975557967\n",
      "37500th epoch, loss: 0.012357023485462491\n",
      "37600th epoch, loss: 0.012153680079230678\n",
      "37700th epoch, loss: 0.012164077944706841\n",
      "37800th epoch, loss: 0.012190347531575059\n",
      "37900th epoch, loss: 0.012048183427211777\n",
      "38000th epoch, loss: 0.012072972468619492\n",
      "38100th epoch, loss: 0.012078355727938861\n",
      "38200th epoch, loss: 0.012058693403037625\n",
      "38300th epoch, loss: 0.012077380616627916\n",
      "38400th epoch, loss: 0.01197344551144924\n",
      "38500th epoch, loss: 0.012095467259317216\n",
      "38600th epoch, loss: 0.012102177729844548\n",
      "38700th epoch, loss: 0.011968429742548305\n",
      "38800th epoch, loss: 0.011911549556334564\n",
      "38900th epoch, loss: 0.012039449427200664\n",
      "39000th epoch, loss: 0.011983351324936671\n",
      "39100th epoch, loss: 0.011958955422879197\n",
      "39200th epoch, loss: 0.012039071856116463\n",
      "39300th epoch, loss: 0.01181617245186564\n",
      "39400th epoch, loss: 0.01190187253444954\n",
      "39500th epoch, loss: 0.011801564715762203\n",
      "39600th epoch, loss: 0.011840998917065299\n",
      "39700th epoch, loss: 0.011996710709999298\n",
      "39800th epoch, loss: 0.011723108465996033\n",
      "39900th epoch, loss: 0.01187273060713534\n",
      "40000th epoch, loss: 0.011759240404212503\n",
      "40100th epoch, loss: 0.01171169028232716\n",
      "40200th epoch, loss: 0.011725901652413596\n",
      "40300th epoch, loss: 0.011643587015688385\n",
      "40400th epoch, loss: 0.011637892011090523\n",
      "40500th epoch, loss: 0.01178286650177929\n",
      "40600th epoch, loss: 0.011589410511347039\n",
      "40700th epoch, loss: 0.011760533849313928\n",
      "40800th epoch, loss: 0.01165043961976692\n",
      "40900th epoch, loss: 0.01171505324523305\n",
      "41000th epoch, loss: 0.01160699853400408\n",
      "41100th epoch, loss: 0.011700010772813914\n",
      "41200th epoch, loss: 0.011554635828799394\n",
      "41300th epoch, loss: 0.011464336475745096\n",
      "41400th epoch, loss: 0.011743100893832297\n",
      "41500th epoch, loss: 0.011587254442920579\n",
      "41600th epoch, loss: 0.011483926839630284\n",
      "41700th epoch, loss: 0.011553119657395026\n",
      "41800th epoch, loss: 0.011503523717684108\n",
      "41900th epoch, loss: 0.011394794310322953\n",
      "42000th epoch, loss: 0.011573080041319076\n",
      "42100th epoch, loss: 0.011349351867745754\n",
      "42200th epoch, loss: 0.011479705394809357\n",
      "42300th epoch, loss: 0.011341403578383391\n",
      "42400th epoch, loss: 0.011317307263752106\n",
      "42500th epoch, loss: 0.011402214744718073\n",
      "42600th epoch, loss: 0.011358503187960955\n",
      "42700th epoch, loss: 0.01133162454827774\n",
      "42800th epoch, loss: 0.011220333364214968\n",
      "42900th epoch, loss: 0.011254729523509132\n",
      "43000th epoch, loss: 0.011338945878267934\n",
      "43100th epoch, loss: 0.011190131329828039\n",
      "43200th epoch, loss: 0.011220816568425699\n",
      "43300th epoch, loss: 0.011256645571198851\n",
      "43400th epoch, loss: 0.011279025712869538\n",
      "43500th epoch, loss: 0.01116955136447649\n",
      "43600th epoch, loss: 0.011227751871787879\n",
      "43700th epoch, loss: 0.011119843937167938\n",
      "43800th epoch, loss: 0.011122635243176386\n",
      "43900th epoch, loss: 0.011365973602104085\n",
      "44000th epoch, loss: 0.011101699461615655\n",
      "44100th epoch, loss: 0.011189653613012622\n",
      "44200th epoch, loss: 0.011131783310006891\n",
      "44300th epoch, loss: 0.011197480805917693\n",
      "44400th epoch, loss: 0.010995708045826589\n",
      "44500th epoch, loss: 0.011091128587109023\n",
      "44600th epoch, loss: 0.01105312057768724\n",
      "44700th epoch, loss: 0.011042906373742545\n",
      "44800th epoch, loss: 0.01099759772563035\n",
      "44900th epoch, loss: 0.011010743383951053\n",
      "45000th epoch, loss: 0.01090106005091109\n",
      "45100th epoch, loss: 0.011042532500096633\n",
      "45200th epoch, loss: 0.010897356844729433\n",
      "45300th epoch, loss: 0.010971660160820292\n",
      "45400th epoch, loss: 0.011025840671084801\n",
      "45500th epoch, loss: 0.010927046111443\n",
      "45600th epoch, loss: 0.010825159534556289\n",
      "45700th epoch, loss: 0.010992668025110702\n",
      "45800th epoch, loss: 0.01083118046503643\n",
      "45900th epoch, loss: 0.01083782207648813\n",
      "46000th epoch, loss: 0.01102908517010296\n",
      "46100th epoch, loss: 0.010759728034952282\n",
      "46200th epoch, loss: 0.010856682524353845\n",
      "46300th epoch, loss: 0.010731541961842539\n",
      "46400th epoch, loss: 0.010918411167351037\n",
      "46500th epoch, loss: 0.010677508819481898\n",
      "46600th epoch, loss: 0.010822866677814768\n",
      "46700th epoch, loss: 0.010669844451062588\n",
      "46800th epoch, loss: 0.01073020488271367\n",
      "46900th epoch, loss: 0.010945268351426124\n",
      "47000th epoch, loss: 0.010729678314093088\n",
      "47100th epoch, loss: 0.010717028624549195\n",
      "47200th epoch, loss: 0.010672841260433986\n",
      "47300th epoch, loss: 0.010587556690232749\n",
      "47400th epoch, loss: 0.010815175534481835\n",
      "47500th epoch, loss: 0.010645726567991528\n",
      "47600th epoch, loss: 0.010667984573438942\n",
      "47700th epoch, loss: 0.010530334042155545\n",
      "47800th epoch, loss: 0.010601665118916904\n",
      "47900th epoch, loss: 0.010510101996536014\n",
      "48000th epoch, loss: 0.010532790160625844\n",
      "48100th epoch, loss: 0.010623811787375804\n",
      "48200th epoch, loss: 0.010456482460424356\n",
      "48300th epoch, loss: 0.010584551740241508\n",
      "48400th epoch, loss: 0.010439998784623426\n",
      "48500th epoch, loss: 0.010631614503724476\n",
      "48600th epoch, loss: 0.010497679375719125\n",
      "48700th epoch, loss: 0.010451752684247596\n",
      "48800th epoch, loss: 0.01039354389132398\n",
      "48900th epoch, loss: 0.010422684076520227\n",
      "49000th epoch, loss: 0.010726577782348534\n",
      "49100th epoch, loss: 0.010483240019319244\n",
      "49200th epoch, loss: 0.01035776183722216\n",
      "49300th epoch, loss: 0.010382743521287135\n",
      "49400th epoch, loss: 0.010299184569427314\n",
      "49500th epoch, loss: 0.01070109119936639\n",
      "49600th epoch, loss: 0.010359795495159349\n",
      "49700th epoch, loss: 0.010314351097302178\n",
      "49800th epoch, loss: 0.010314177600555821\n",
      "49900th epoch, loss: 0.010417050844684809\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# ======================================\n",
    "# Prepare Data\n",
    "def make_batch(data, batch_size, window_size, shuffle=True):\n",
    "    window_list = []\n",
    "    for i in range(len(data) - window_size - 1):\n",
    "        window = data[i: i + window_size]\n",
    "        window_list.append(window)\n",
    "#데이터에서 사이즈씩 뽑아서 리스트를 만들어줌...\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(window_list)\n",
    "#셔플은 기본적으로 필요함\n",
    "\n",
    "#윈도우 리스트는?\n",
    "#윈도우 사이즈 - 열 숫자? \n",
    "#배치 사이즈 - 한번에 넘겨주는 데이터 사이즈-몇개의윈도우를 한번에 넘겨줄것인가.\n",
    "    n_batch = math.ceil(len(window_list) / batch_size)\n",
    "    batch_list = []\n",
    "    for i in range(n_batch): #100에 10개씩이면 10번만\n",
    "        batch = window_list[i*batch_size: (i+1)*batch_size] #0~9\n",
    "        batch_list.append(batch)\n",
    "    batch_list = np.array(batch_list)\n",
    "\n",
    "    return batch_list\n",
    "\n",
    "data = pd.read_csv(\"./10feature_cpu.csv\")\n",
    "col = list(map(str, data.columns))\n",
    "data = data[col[:-1]]\n",
    "data = data.to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dscaler = scaler.fit(data)\n",
    "data = dscaler.transform(data)\n",
    "\n",
    "# ========================================\n",
    "# Modeling\n",
    "class SequenceModel(nn.Module): # 아웃풋이 4라는건 4개의 값을 모두 예측? #히든 사이즈의 정의 # 넘레이어..? 보통 사용하는 숫자..찾아봐야 함..\n",
    "    def __init__(self, input_size=4, output_dim=4, hidden_size=256, num_layers=1):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.scaler_bias = nn.Parameter(torch.ones(input_size, requires_grad=True))\n",
    "        self.scaler = nn.Parameter(torch.ones(input_size, requires_grad=True))\n",
    "        self.linear = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x + self.scaler_bias) * self.scaler #식은 어디서 확인 가능..?\n",
    "        zs, hidden = self.lstm(x)\n",
    "        z = zs[:, -1]\n",
    "        v = self.linear(zs)\n",
    "        return v, z\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Training\n",
    "window_size = 80\n",
    "batch_size = 2048\n",
    "hidden_size = 20\n",
    "use_cuda = True\n",
    "\n",
    "model = SequenceModel(input_size=data.shape[-1],\n",
    "                      output_dim=data.shape[-1],\n",
    "                      hidden_size=hidden_size,\n",
    "                      num_layers=5)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "n_epoch = 50000\n",
    "ema_loss = None\n",
    "alpha = 0.1\n",
    "verbose_interval = 100\n",
    "\n",
    "for epoch_i in range(n_epoch):\n",
    "\n",
    "    batch_list = make_batch(data, batch_size, window_size+1)\n",
    "    for batch_i, batch in enumerate(batch_list):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = np.array(batch)\n",
    "        batch_input = batch[:, :-1, :] #3차?\n",
    "        batch_output = batch[:, 1:, :]\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.float32)\n",
    "        batch_output = torch.tensor(batch_output, dtype=torch.float32)\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_output = batch_output.cuda()\n",
    "\n",
    "\n",
    "        v, _ = model(batch_input)\n",
    "\n",
    "        loss = loss_fn(v, batch_output) #output, Target 로스값 계산. 해서 다시넣는 과정..\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ema_loss is None:\n",
    "            ema_loss = loss.item()\n",
    "        ema_loss = loss.item() * alpha + (1.-alpha) * ema_loss\n",
    "\n",
    "    if epoch_i % verbose_interval == 0:\n",
    "        print(f\"{epoch_i}th epoch, loss: {ema_loss}\")\n",
    "        #print(v.shape)\n",
    "        #print(v)\n",
    "        #print()\n",
    "\n",
    "#만번 학습시킨다음에 마지막 v값 한번 봐보자\n",
    "#윈도우 배치 히든 n_lay\n",
    "#5 512 10 10 8.8\n",
    "#20 2048 20 20 - 20 20 이 문제가있는듯 -> num_layer가 너무 높으면 학습이안됨. 히든사이즈는 ok 근데또 num_l 1보단 10이 성능이좋음 5는 더좋네(만일때 3프로대진입) \n",
    "#\" 10 10 6.8\n",
    "# n이 3일때 1.8퍼까지. 7은 2퍼  2는 3퍼 6은 1.95 !4가 1.6퍼 & 5도 1.6퍼  8은 2.2..\n",
    "#n5에 윈도우 40주니 1.1퍼 윈60 1퍼  윈 80 1퍼 윈100 0.9퍼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Inference\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "\n",
    "# Prepare train data distribution\n",
    "#Z는 학습 데이터를 모델에 태워서 Z를 만들어냄 레이턴트 벡터 .. 레이턴트 백터.. 학습데이터 \n",
    "Z = []\n",
    "reconstruction_error = []\n",
    "\n",
    "batch_list = make_batch(data, batch_size, window_size, False)\n",
    "for batch_i, batch in enumerate(batch_list):\n",
    "    batch = np.array(batch)\n",
    "    batch_input = batch\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.float32)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.float32)\n",
    "\n",
    "    v, z = model(batch_input) #레이턴트 백터 \n",
    "\n",
    "    Z.extend(z.tolist())  #리스트로 변환해서 Z에 추가/\n",
    "    reconstruction_error.extend(torch.sum(torch.abs(v-batch_input), dim=[1,2]).detach().tolist())\n",
    "\n",
    "Z = np.array(Z)\n",
    "reconstruction_error = np.array(reconstruction_error)\n",
    "\n",
    "\n",
    "# Samples for quering\n",
    "sample_pos = [[5.1, 20.5,  1.0,  4.9],\n",
    "              [4.1, 16.3,  1.0,  6.1],\n",
    "              [9.1, 36.5,  1.0,  2.7],\n",
    "              [2.3,  9.2,  1.0, 10.9],\n",
    "              [1.6,  6.4,  1.0, 15.7],\n",
    "              [6.6, 26.3,  1.0,  3.8],\n",
    "              [8.0, 31.9,  1.0,  3.1],\n",
    "              [7.8, 31.1,  1.0,  3.2],\n",
    "              [7.0,  28.,  1.0,  3.6],\n",
    "              [7.0,  28.,  1.0,  3.6]]\n",
    "\n",
    "\n",
    "sample_neg = [[ 66, 267,   5,   0],\n",
    "                [ 74, 298,   5,   0],\n",
    "                [ 88, 354,   5,   0],\n",
    "              [ 83, 335,   5,   0],\n",
    "              [ 78, 315,   5,   0],\n",
    "              [ 96, 385,   5,   0],\n",
    "              [ 15,  59,   5,   1],\n",
    "              [ 67, 267,   5,   0],\n",
    "              [ 75, 303,   5,   0],\n",
    "              [ 60, 242,   5,   0]]\n",
    "\n",
    "# pos\n",
    "sample_pos = np.array(sample_pos)  # sequence_length x feature size\n",
    "sample_pos = torch.tensor(sample_pos, dtype=torch.float32)  # sequence_length x feature size\n",
    "sample_pos = sample_pos.unsqueeze(0)  # 1 x sequence_length x feature size\n",
    "prediction_pos, z_prime_pos = model(sample_pos)\n",
    "\n",
    "# neg\n",
    "sample_neg = np.array(sample_neg)  # sequence_length x feature size\n",
    "sample_neg = torch.tensor(sample_neg, dtype=torch.float32)  # sequence_length x feature size\n",
    "sample_neg = sample_neg.unsqueeze(0)  # 1 x sequence_length x feature size\n",
    "prediction_neg, z_prime_neg = model(sample_neg)\n",
    "\n",
    "z_prime_pos = z_prime_pos.detach().numpy()\n",
    "z_prime_neg = z_prime_neg.detach().numpy()\n",
    "\n",
    "reconstruction_error_pos = torch.sum(torch.abs(prediction_pos - sample_pos), dim=[1,2]).detach().tolist()\n",
    "reconstruction_error_neg = torch.sum(torch.abs(prediction_neg - sample_neg), dim=[1,2]).detach().tolist()\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Visualize latent space\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(Z)\n",
    "\n",
    "Z_2d = pca.transform(Z)\n",
    "\n",
    "z_prime_pos_2d = pca.transform(z_prime_pos)\n",
    "z_prime_neg_2d = pca.transform(z_prime_neg)\n",
    "\n",
    "plt.scatter(Z_2d[:, 0], Z_2d[:, 1], color='k')\n",
    "plt.scatter(z_prime_pos_2d[:, 0],z_prime_pos_2d[:, 1] , color='g', label='normal')\n",
    "plt.scatter(z_prime_neg_2d[:, 0],z_prime_neg_2d[:, 1] , color='r', label='abnormal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Plot Reconstruction Error \n",
    "neg_height = 50\n",
    "min_val = min(min(reconstruction_error), min(reconstruction_error_neg))\n",
    "max_val = max(max(reconstruction_error), max(reconstruction_error_neg))\n",
    "bins = np.linspace(min_val, \n",
    "                   max_val,\n",
    "                   100)\n",
    "\n",
    "plt.hist(reconstruction_error_neg * neg_height, bins=bins, alpha=0.5, color='red', label='abnormal')\n",
    "plt.hist(reconstruction_error, bins=bins, alpha=0.5,color='k', label='normal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#LSTM으로 그룹2개를 생성함.. 정상과 비정상. \n",
    "#새로운 데이터가 정상 범주인지 비정상범주인지 확인하려면?\n",
    "\n",
    "#out_dim 값이랑 스케일링 range 설정좀 바꿔보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
