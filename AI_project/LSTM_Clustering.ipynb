{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([64, 5, 1796])) that is different to the input size (torch.Size([64, 5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch, loss: 0.06696524372521967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b3bc0fb2fe1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mbatch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b3bc0fb2fe1f>\u001b[0m in \u001b[0;36mmake_batch\u001b[0;34m(data, batch_size, window_size, shuffle)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#0~9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbatch_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbatch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# ======================================\n",
    "# Prepare Data\n",
    "def make_batch(data, batch_size, window_size, shuffle=True):\n",
    "    window_list = []\n",
    "    for i in range(len(data) - window_size - 1):\n",
    "        window = data[i: i + window_size]\n",
    "        window_list.append(window)\n",
    "#데이터에서 사이즈씩 뽑아서 리스트를 만들어줌...\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(window_list)\n",
    "#셔플은 기본적으로 필요함\n",
    "\n",
    "#윈도우 리스트는?\n",
    "#윈도우 사이즈 - 열 숫자? \n",
    "#배치 사이즈 - 한번에 넘겨주는 데이터 사이즈-몇개의윈도우를 한번에 넘겨줄것인가.\n",
    "    n_batch = math.ceil(len(window_list) / batch_size)\n",
    "    batch_list = []\n",
    "    for i in range(n_batch): #100에 10개씩이면 10번만\n",
    "        batch = window_list[i*batch_size: (i+1)*batch_size] #0~9\n",
    "        batch_list.append(batch)\n",
    "    batch_list = np.array(batch_list)\n",
    "\n",
    "    return batch_list\n",
    "\n",
    "data = pd.read_csv(\"./nomal-data_1.csv\")\n",
    "data = data.to_numpy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "dscaler = scaler.fit(data)\n",
    "data = dscaler.transform(data)\n",
    "\n",
    "# ========================================\n",
    "# Modeling\n",
    "class SequenceModel(nn.Module): # 아웃풋이 4라는건 4개의 값을 모두 예측? #히든 사이즈의 정의 # 넘레이어..? 보통 사용하는 숫자..찾아봐야 함..\n",
    "    def __init__(self, input_size=4, output_dim=4, hidden_size=256, num_layers=1):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.scaler_bias = nn.Parameter(torch.ones(input_size, requires_grad=True))\n",
    "        self.scaler = nn.Parameter(torch.ones(input_size, requires_grad=True))\n",
    "        self.linear = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x + self.scaler_bias) * self.scaler #식은 어디서 확인 가능..?\n",
    "        zs, hidden = self.lstm(x)\n",
    "        z = zs[:, -1]\n",
    "        v = self.linear(zs)\n",
    "        return v, z\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Training\n",
    "window_size = 5\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "use_cuda = True\n",
    "\n",
    "model = SequenceModel(input_size=data.shape[-1],\n",
    "                      output_dim=1,\n",
    "                      hidden_size=hidden_size,\n",
    "                      num_layers=7)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "n_epoch = 10000\n",
    "ema_loss = None\n",
    "alpha = 0.1\n",
    "verbose_interval = 50\n",
    "\n",
    "for epoch_i in range(n_epoch):\n",
    "\n",
    "    batch_list = make_batch(data, batch_size, window_size+1)\n",
    "    for batch_i, batch in enumerate(batch_list):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = np.array(batch) #batch는 하나의 batch덩어리. 윈도우들의 모임\n",
    "        batch_input = batch[:, :-1, :] #3차? 윈도우, 타임스탬프, 피처\n",
    "        batch_output = batch[:, 1:, :]\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.float32)\n",
    "        batch_output = torch.tensor(batch_output, dtype=torch.float32)\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_output = batch_output.cuda()\n",
    "\n",
    "\n",
    "        v, _ = model(batch_input)\n",
    "\n",
    "        loss = loss_fn(v, batch_output) #output, Target 로스값 계산. 해서 다시넣는 과정..\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ema_loss is None:\n",
    "            ema_loss = loss.item()\n",
    "        ema_loss = loss.item() * alpha + (1.-alpha) * ema_loss\n",
    "\n",
    "    if epoch_i % verbose_interval == 0:\n",
    "        print(f\"{epoch_i}th epoch, loss: {ema_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Inference\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "\n",
    "# Prepare train data distribution\n",
    "#Z는 학습 데이터를 모델에 태워서 Z를 만들어냄 레이턴트 벡터 .. 레이턴트 백터.. 학습데이터 \n",
    "Z = []\n",
    "reconstruction_error = []\n",
    "\n",
    "batch_list = make_batch(data, batch_size, window_size, False)\n",
    "for batch_i, batch in enumerate(batch_list):\n",
    "    batch = np.array(batch)\n",
    "    batch_input = batch\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.float32)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.float32)\n",
    "\n",
    "    v, z = model(batch_input) #레이턴트 백터 \n",
    "\n",
    "    Z.extend(z.tolist())  #리스트로 변환해서 Z에 추가/\n",
    "    reconstruction_error.extend(torch.sum(torch.abs(v-batch_input), dim=[1,2]).detach().tolist())\n",
    "\n",
    "Z = np.array(Z)\n",
    "reconstruction_error = np.array(reconstruction_error)\n",
    "\n",
    "\n",
    "# Samples for quering\n",
    "sample_pos = [[5.1, 20.5,  1.0,  4.9],\n",
    "              [4.1, 16.3,  1.0,  6.1],\n",
    "              [9.1, 36.5,  1.0,  2.7],\n",
    "              [2.3,  9.2,  1.0, 10.9],\n",
    "              [1.6,  6.4,  1.0, 15.7],\n",
    "              [6.6, 26.3,  1.0,  3.8],\n",
    "              [8.0, 31.9,  1.0,  3.1],\n",
    "              [7.8, 31.1,  1.0,  3.2],\n",
    "              [7.0,  28.,  1.0,  3.6],\n",
    "              [7.0,  28.,  1.0,  3.6]]\n",
    "\n",
    "\n",
    "sample_neg = [[ 66, 267,   5,   0],\n",
    "                [ 74, 298,   5,   0],\n",
    "                [ 88, 354,   5,   0],\n",
    "              [ 83, 335,   5,   0],\n",
    "              [ 78, 315,   5,   0],\n",
    "              [ 96, 385,   5,   0],\n",
    "              [ 15,  59,   5,   1],\n",
    "              [ 67, 267,   5,   0],\n",
    "              [ 75, 303,   5,   0],\n",
    "              [ 60, 242,   5,   0]]\n",
    "\n",
    "# pos\n",
    "sample_pos = np.array(sample_pos)  # sequence_length x feature size\n",
    "sample_pos = torch.tensor(sample_pos, dtype=torch.float32)  # sequence_length x feature size\n",
    "sample_pos = sample_pos.unsqueeze(0)  # 1 x sequence_length x feature size\n",
    "prediction_pos, z_prime_pos = model(sample_pos)\n",
    "\n",
    "# neg\n",
    "sample_neg = np.array(sample_neg)  # sequence_length x feature size\n",
    "sample_neg = torch.tensor(sample_neg, dtype=torch.float32)  # sequence_length x feature size\n",
    "sample_neg = sample_neg.unsqueeze(0)  # 1 x sequence_length x feature size\n",
    "prediction_neg, z_prime_neg = model(sample_neg)\n",
    "\n",
    "z_prime_pos = z_prime_pos.detach().numpy()\n",
    "z_prime_neg = z_prime_neg.detach().numpy()\n",
    "\n",
    "reconstruction_error_pos = torch.sum(torch.abs(prediction_pos - sample_pos), dim=[1,2]).detach().tolist()\n",
    "reconstruction_error_neg = torch.sum(torch.abs(prediction_neg - sample_neg), dim=[1,2]).detach().tolist()\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Visualize latent space\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(Z)\n",
    "\n",
    "Z_2d = pca.transform(Z)\n",
    "\n",
    "z_prime_pos_2d = pca.transform(z_prime_pos)\n",
    "z_prime_neg_2d = pca.transform(z_prime_neg)\n",
    "\n",
    "plt.scatter(Z_2d[:, 0], Z_2d[:, 1], color='k')\n",
    "plt.scatter(z_prime_pos_2d[:, 0],z_prime_pos_2d[:, 1] , color='g', label='normal')\n",
    "plt.scatter(z_prime_neg_2d[:, 0],z_prime_neg_2d[:, 1] , color='r', label='abnormal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# Plot Reconstruction Error \n",
    "neg_height = 50\n",
    "min_val = min(min(reconstruction_error), min(reconstruction_error_neg))\n",
    "max_val = max(max(reconstruction_error), max(reconstruction_error_neg))\n",
    "bins = np.linspace(min_val, \n",
    "                   max_val,\n",
    "                   100)\n",
    "\n",
    "plt.hist(reconstruction_error_neg * neg_height, bins=bins, alpha=0.5, color='red', label='abnormal')\n",
    "plt.hist(reconstruction_error, bins=bins, alpha=0.5,color='k', label='normal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#LSTM으로 그룹2개를 생성함.. 정상과 비정상. \n",
    "#새로운 데이터가 정상 범주인지 비정상범주인지 확인하려면?\n",
    "\n",
    "#out_dim 값이랑 스케일링 range 설정좀 바꿔보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
